{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda1 1: Instalar Librerias\n",
    "!pip install --quiet boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 2: Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import boto3\n",
    "import yaml\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 3: Cargar Credenciales\n",
    "with open(\"credentials.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 4: Funcion para cargar modelo\n",
    "def load_model(bucket, bucket_path):\n",
    "    \"\"\"\n",
    "    Load a model from S3 bucket\n",
    "    \n",
    "    Parameters:\n",
    "    bucket (str): Name of the S3 bucket\n",
    "    bucket_path (str): Path to the model file in the bucket\n",
    "    \n",
    "    Returns:\n",
    "    object: Loaded model\n",
    "    \"\"\"\n",
    "    session = boto3.Session(\n",
    "        aws_access_key_id=config['s3']['aws_access_key_id'],\n",
    "        aws_secret_access_key=config['s3']['aws_secret_access_key'],\n",
    "        aws_session_token=config['s3']['aws_session_token']\n",
    "    )\n",
    "    \n",
    "    s3 = session.resource('s3')\n",
    "    obj = s3.Object(bucket, bucket_path).get()['Body'].read()\n",
    "    model = pickle.loads(obj)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 5: Cargar Modelo y plotear ROC\n",
    "def plot_roc_curve(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Generate ROC curve and calculate AUC\n",
    "    \n",
    "    Parameters:\n",
    "    model: Trained model\n",
    "    X_test: Test features\n",
    "    y_test: Test labels\n",
    "    \n",
    "    Returns:\n",
    "    float: AUC score\n",
    "    \"\"\"\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 6: Generar Metricas\n",
    "def generate_metrics_table(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Generate comprehensive metrics table\n",
    "    \n",
    "    Parameters:\n",
    "    model: Trained model\n",
    "    X_test: Test features\n",
    "    y_test: Test labels\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Metrics table\n",
    "    \"\"\"\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "    \n",
    "    metrics_list = []\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        \n",
    "        tnr = tn / (tn + fp)  # Specificity\n",
    "        fnr = fn / (fn + tp)  # Miss rate\n",
    "        prec = tp / (tp + fp) if (tp + fp) > 0 else 0  # Precision\n",
    "        rec = tp / (tp + fn)  # Recall\n",
    "        f1 = 2 * (prec * rec) / (prec + rec) if (prec + rec) > 0 else 0  # F1 score\n",
    "        \n",
    "        metrics_list.append({\n",
    "            'threshold': threshold,\n",
    "            'tnr': tnr,\n",
    "            'fnr': fnr,\n",
    "            'precision': prec,\n",
    "            'recall': rec,\n",
    "            'f1_score': f1\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(metrics_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 7: Traer Metricas de S3\n",
    "def get_metrics(bucket, metrics_path):\n",
    "    \"\"\"\n",
    "    Load metrics from S3\n",
    "    \n",
    "    Parameters:\n",
    "    bucket (str): Name of the S3 bucket\n",
    "    metrics_path (str): Path to the metrics file\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Metrics table\n",
    "    \"\"\"\n",
    "    return load_model(bucket, metrics_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 8: FUncion para elegir el mejor modelo\n",
    "def select_best_model(min_recall, metrics_paths):\n",
    "    \"\"\"\n",
    "    Select the best model based on business constraints\n",
    "    \n",
    "    Parameters:\n",
    "    min_recall (float): Minimum recall threshold\n",
    "    metrics_paths (list): List of paths to metrics files\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (model_name, threshold, metrics)\n",
    "    \"\"\"\n",
    "    best_f1 = -1\n",
    "    best_model = None\n",
    "    best_threshold = None\n",
    "    best_metrics = None\n",
    "    \n",
    "    for path in metrics_paths:\n",
    "        metrics = get_metrics(bucket, path)\n",
    "        valid_metrics = metrics[metrics['recall'] >= min_recall]\n",
    "        \n",
    "        if not valid_metrics.empty:\n",
    "            max_f1_idx = valid_metrics['f1_score'].idxmax()\n",
    "            if valid_metrics.loc[max_f1_idx, 'f1_score'] > best_f1:\n",
    "                best_f1 = valid_metrics.loc[max_f1_idx, 'f1_score']\n",
    "                best_model = path.split('/')[-1].replace('metricas_', '').replace('.pkl', '')\n",
    "                best_threshold = valid_metrics.loc[max_f1_idx, 'threshold']\n",
    "                best_metrics = valid_metrics.loc[max_f1_idx]\n",
    "    \n",
    "    return (best_model, best_threshold, best_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 9: FUnciòn para guardar Mejor Modelo\n",
    "def save_threshold_best_model(best_model_tuple, bucket):\n",
    "    \"\"\"\n",
    "    Save the best model threshold\n",
    "    \n",
    "    Parameters:\n",
    "    best_model_tuple (tuple): Output from select_best_model\n",
    "    bucket (str): Name of the S3 bucket\n",
    "    \"\"\"\n",
    "    session = boto3.Session(\n",
    "        aws_access_key_id=config['s3']['aws_access_key_id'],\n",
    "        aws_secret_access_key=config['s3']['aws_secret_access_key'],\n",
    "        aws_session_token=config['s3']['aws_session_token']\n",
    "    )\n",
    "    \n",
    "    s3 = session.resource('s3')\n",
    "    pickle_data = pickle.dumps(best_model_tuple)\n",
    "    s3.Object(bucket, 'best_model/threshold.pkl').put(Body=pickle_data)\n",
    "    \n",
    "    ## Función para guardar modelos en S3\n",
    "def save_model(bucket, bucket_path, model):\n",
    "    session = boto3.Session(\n",
    "        aws_access_key_id=config['s3']['aws_access_key_id'],\n",
    "        aws_secret_access_key=config['s3']['aws_secret_access_key'],\n",
    "        aws_session_token=config['s3']['aws_session_token']\n",
    "    )\n",
    "\n",
    "    s3 = session.resource('s3')\n",
    "    pickle_data = pickle.dumps(model)\n",
    "    s3.Object(bucket, bucket_path).put(Body=pickle_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 11: Guardar Mejor Modelo\n",
    "def save_best_model(bucket, model_path, model):\n",
    "    \"\"\"\n",
    "    Save the selected model\n",
    "    \n",
    "    Parameters:\n",
    "    bucket (str): Name of the S3 bucket\n",
    "    model_path (str): Path where to save the model\n",
    "    model: Model to save\n",
    "    \"\"\"\n",
    "    session = boto3.Session(\n",
    "        aws_access_key_id=config['s3']['aws_access_key_id'],\n",
    "        aws_secret_access_key=config['s3']['aws_secret_access_key'],\n",
    "        aws_session_token=config['s3']['aws_session_token']\n",
    "    )\n",
    "    \n",
    "    s3 = session.resource('s3')\n",
    "    pickle_data = pickle.dumps(model)\n",
    "    s3.Object(bucket, model_path).put(Body=pickle_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos de prueba...\n",
      "Datos de prueba cargados correctamente\n",
      "Carpeta de evaluación creada\n",
      "\n",
      "Procesando modelo: arbol\n",
      "Modelo arbol cargado correctamente\n",
      "Generando métricas para arbol...\n",
      "Métricas guardadas para arbol\n",
      "\n",
      "Proceso de generación de métricas completado\n",
      "\n",
      "Seleccionando mejor modelo...\n",
      "Threshold del mejor modelo guardado\n",
      "Modelo seleccionado guardado exitosamente\n"
     ]
    }
   ],
   "source": [
    "# Celda 12\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# bucket\n",
    "bucket = \"aplicaciones-cd-2-\" + config['iexe']['matricula']\n",
    "\n",
    "print(\"Cargando datos de prueba...\")\n",
    "# Cargar test data\n",
    "test_data = load_model(bucket, \"dataset/test/test_dataset.pkl\")\n",
    "X_test = test_data.iloc[:, :-1]\n",
    "y_test = test_data.iloc[:, -1]\n",
    "\n",
    "# Convertir etiquetas categóricas a numéricas\n",
    "le = LabelEncoder()\n",
    "y_test_encoded = le.fit_transform(y_test)\n",
    "print(\"Datos de prueba cargados correctamente\")\n",
    "\n",
    "# Crear carpeta de evaluación\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=config['s3']['aws_access_key_id'],\n",
    "    aws_secret_access_key=config['s3']['aws_secret_access_key'],\n",
    "    aws_session_token=config['s3']['aws_session_token']\n",
    ")\n",
    "s3 = session.resource('s3')\n",
    "\n",
    "try:\n",
    "    s3.Object(bucket, 'evaluacion/').put(Body='')\n",
    "    print(\"Carpeta de evaluación creada\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"La carpeta de evaluación ya existe o hubo un error: {e}\\n\")\n",
    "\n",
    "# Cargar y evaluar modelos\n",
    "models = {\n",
    "    'arbol': 'models/decision_tree_best_model.pkl'\n",
    "}\n",
    "\n",
    "def generate_metrics_table_modified(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Generate comprehensive metrics table with handling for categorical labels\n",
    "    \"\"\"\n",
    "    # Obtener probabilidades de predicción\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calcular precision-recall para diferentes umbrales\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba, pos_label=1)\n",
    "    \n",
    "    metrics_list = []\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        \n",
    "        tnr = tn / (tn + fp) if (tn + fp) > 0 else 0  # Specificity\n",
    "        fnr = fn / (fn + tp) if (fn + tp) > 0 else 0  # Miss rate\n",
    "        prec = tp / (tp + fp) if (tp + fp) > 0 else 0  # Precision\n",
    "        rec = tp / (tp + fn) if (tp + fn) > 0 else 0  # Recall\n",
    "        f1 = 2 * (prec * rec) / (prec + rec) if (prec + rec) > 0 else 0  # F1 score\n",
    "        \n",
    "        metrics_list.append({\n",
    "            'threshold': threshold,\n",
    "            'tnr': tnr,\n",
    "            'fnr': fnr,\n",
    "            'precision': prec,\n",
    "            'recall': rec,\n",
    "            'f1_score': f1\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(metrics_list)\n",
    "\n",
    "# Procesar cada modelo\n",
    "metrics_generated = False\n",
    "for model_name, model_path in models.items():\n",
    "    try:\n",
    "        print(f\"Procesando modelo: {model_name}\")\n",
    "        # Cargar modelo\n",
    "        model = load_model(bucket, model_path)\n",
    "        print(f\"Modelo {model_name} cargado correctamente\")\n",
    "        \n",
    "        print(f\"Generando métricas para {model_name}...\")\n",
    "        # Generar y Guardar Métricas\n",
    "        metrics = generate_metrics_table_modified(model, X_test, y_test_encoded)\n",
    "        save_model(bucket, f\"evaluacion/metricas_{model_name}.pkl\", metrics)\n",
    "        print(f\"Métricas guardadas para {model_name}\\n\")\n",
    "        metrics_generated = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando {model_name}: {str(e)}\\n\")\n",
    "\n",
    "print(\"Proceso de generación de métricas completado\")\n",
    "\n",
    "if not metrics_generated:\n",
    "    print(\"No se pudieron generar métricas para ningún modelo\")\n",
    "    exit()\n",
    "\n",
    "# Seleccionar mejor Modelo\n",
    "print(\"\\nSeleccionando mejor modelo...\")\n",
    "metrics_paths = [\n",
    "    \"evaluacion/metricas_arbol.pkl\"\n",
    "]\n",
    "\n",
    "best_model_info = select_best_model(0.5, metrics_paths)\n",
    "\n",
    "if best_model_info:\n",
    "    # Guardar threshold\n",
    "    save_threshold_best_model(best_model_info, bucket)\n",
    "    print(\"Threshold del mejor modelo guardado\")\n",
    "\n",
    "    # Guardar Modelo Seleccionado\n",
    "    try:\n",
    "        selected_model = load_model(bucket, f\"models/decision_tree_best_model.pkl\")\n",
    "        save_best_model(bucket, \"selected-model/selected_model.pkl\", selected_model)\n",
    "        print(\"Modelo seleccionado guardado exitosamente\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error guardando el modelo seleccionado: {str(e)}\")\n",
    "else:\n",
    "    print(\"No se encontró un modelo que cumpla con los criterios mínimos de recall\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
