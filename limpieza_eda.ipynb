{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza y Análisis Exploratorio de Datos (EDA)\n",
    "\n",
    "Este notebook realiza una limpieza básica de los datos ingestados a través de la API y un análisis exploratorio de las inspecciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import pickle\n",
    "import yaml\n",
    "from datetime import date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cargar datos de S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se cargaron 280438 registros de S3.\n"
     ]
    }
   ],
   "source": [
    "# Cargar configuración\n",
    "with open(\"credentials.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "def cargar_datos_s3(bucket, key_prefix):\n",
    "    session = boto3.Session(\n",
    "        aws_access_key_id=config['s3']['aws_access_key_id'],\n",
    "        aws_secret_access_key=config['s3']['aws_secret_access_key'],\n",
    "        aws_session_token=config['s3']['aws_session_token']\n",
    "    )\n",
    "    s3 = session.client('s3')\n",
    "    \n",
    "    # Obtener la lista de objetos en el bucket con el prefijo dado\n",
    "    response = s3.list_objects_v2(Bucket=bucket, Prefix=key_prefix)\n",
    "    \n",
    "    # Obtener la clave del archivo más reciente\n",
    "    latest_file = max(response['Contents'], key=lambda x: x['LastModified'])['Key']\n",
    "    \n",
    "    # Descargar el objeto\n",
    "    obj = s3.get_object(Bucket=bucket, Key=latest_file)\n",
    "    \n",
    "    # Cargar los datos del pickle\n",
    "    dataset = pickle.loads(obj['Body'].read())\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Configurar el bucket y la clave\n",
    "bucket = \"aplicaciones-cd-1-\" + config['iexe']['matricula']\n",
    "key_prefix = \"ingesta/inicial/\"\n",
    "\n",
    "# Cargar los datos\n",
    "dataset = cargar_datos_s3(bucket, key_prefix)\n",
    "\n",
    "print(f\"Se cargaron {len(dataset)} registros de S3.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transformar ingesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame creado con 280438 filas y 17 columnas.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inspection_id</th>\n",
       "      <th>dba_name</th>\n",
       "      <th>aka_name</th>\n",
       "      <th>license_</th>\n",
       "      <th>facility_type</th>\n",
       "      <th>risk</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>inspection_date</th>\n",
       "      <th>inspection_type</th>\n",
       "      <th>results</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>location</th>\n",
       "      <th>violations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67732</td>\n",
       "      <td>WOLCOTT'S</td>\n",
       "      <td>TROQUET</td>\n",
       "      <td>1992039</td>\n",
       "      <td>Restaurant</td>\n",
       "      <td>Risk 1 (High)</td>\n",
       "      <td>1834 W MONTROSE AVE</td>\n",
       "      <td>CHICAGO</td>\n",
       "      <td>IL</td>\n",
       "      <td>60613</td>\n",
       "      <td>2010-01-04T00:00:00.000</td>\n",
       "      <td>License Re-Inspection</td>\n",
       "      <td>Pass</td>\n",
       "      <td>41.961605669949854</td>\n",
       "      <td>-87.67596676683779</td>\n",
       "      <td>{'latitude': '41.961605669949854', 'longitude'...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67738</td>\n",
       "      <td>MICHAEL'S ON MAIN CAFE</td>\n",
       "      <td>MICHAEL'S ON MAIN CAFE</td>\n",
       "      <td>2008948</td>\n",
       "      <td>Restaurant</td>\n",
       "      <td>Risk 1 (High)</td>\n",
       "      <td>8750 W BRYN WAWR AVE</td>\n",
       "      <td>CHICAGO</td>\n",
       "      <td>IL</td>\n",
       "      <td>60631</td>\n",
       "      <td>2010-01-04T00:00:00.000</td>\n",
       "      <td>License</td>\n",
       "      <td>Fail</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18. NO EVIDENCE OF RODENT OR INSECT OUTER OPEN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67733</td>\n",
       "      <td>WOLCOTT'S</td>\n",
       "      <td>TROQUET</td>\n",
       "      <td>1992040</td>\n",
       "      <td>Restaurant</td>\n",
       "      <td>Risk 1 (High)</td>\n",
       "      <td>1834 W MONTROSE AVE</td>\n",
       "      <td>CHICAGO</td>\n",
       "      <td>IL</td>\n",
       "      <td>60613</td>\n",
       "      <td>2010-01-04T00:00:00.000</td>\n",
       "      <td>License Re-Inspection</td>\n",
       "      <td>Pass</td>\n",
       "      <td>41.961605669949854</td>\n",
       "      <td>-87.67596676683779</td>\n",
       "      <td>{'latitude': '41.961605669949854', 'longitude'...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104236</td>\n",
       "      <td>TEMPO CAFE</td>\n",
       "      <td>TEMPO CAFE</td>\n",
       "      <td>80916</td>\n",
       "      <td>Restaurant</td>\n",
       "      <td>Risk 1 (High)</td>\n",
       "      <td>6 E CHESTNUT ST</td>\n",
       "      <td>CHICAGO</td>\n",
       "      <td>IL</td>\n",
       "      <td>60611</td>\n",
       "      <td>2010-01-04T00:00:00.000</td>\n",
       "      <td>Canvass</td>\n",
       "      <td>Fail</td>\n",
       "      <td>41.89843137207629</td>\n",
       "      <td>-87.6280091630558</td>\n",
       "      <td>{'latitude': '41.89843137207629', 'longitude':...</td>\n",
       "      <td>18. NO EVIDENCE OF RODENT OR INSECT OUTER OPEN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70269</td>\n",
       "      <td>mr.daniel's</td>\n",
       "      <td>mr.daniel's</td>\n",
       "      <td>1899292</td>\n",
       "      <td>Restaurant</td>\n",
       "      <td>Risk 1 (High)</td>\n",
       "      <td>5645 W BELMONT AVE</td>\n",
       "      <td>CHICAGO</td>\n",
       "      <td>IL</td>\n",
       "      <td>60634</td>\n",
       "      <td>2010-01-04T00:00:00.000</td>\n",
       "      <td>License Re-Inspection</td>\n",
       "      <td>Pass</td>\n",
       "      <td>41.93844282365204</td>\n",
       "      <td>-87.76831838068422</td>\n",
       "      <td>{'latitude': '41.93844282365204', 'longitude':...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  inspection_id                dba_name                aka_name license_  \\\n",
       "0         67732               WOLCOTT'S                 TROQUET  1992039   \n",
       "1         67738  MICHAEL'S ON MAIN CAFE  MICHAEL'S ON MAIN CAFE  2008948   \n",
       "2         67733               WOLCOTT'S                 TROQUET  1992040   \n",
       "3        104236              TEMPO CAFE              TEMPO CAFE    80916   \n",
       "4         70269             mr.daniel's             mr.daniel's  1899292   \n",
       "\n",
       "  facility_type           risk                address     city state    zip  \\\n",
       "0    Restaurant  Risk 1 (High)   1834 W MONTROSE AVE   CHICAGO    IL  60613   \n",
       "1    Restaurant  Risk 1 (High)  8750 W BRYN WAWR AVE   CHICAGO    IL  60631   \n",
       "2    Restaurant  Risk 1 (High)   1834 W MONTROSE AVE   CHICAGO    IL  60613   \n",
       "3    Restaurant  Risk 1 (High)       6 E CHESTNUT ST   CHICAGO    IL  60611   \n",
       "4    Restaurant  Risk 1 (High)    5645 W BELMONT AVE   CHICAGO    IL  60634   \n",
       "\n",
       "           inspection_date        inspection_type results            latitude  \\\n",
       "0  2010-01-04T00:00:00.000  License Re-Inspection    Pass  41.961605669949854   \n",
       "1  2010-01-04T00:00:00.000                License    Fail                 NaN   \n",
       "2  2010-01-04T00:00:00.000  License Re-Inspection    Pass  41.961605669949854   \n",
       "3  2010-01-04T00:00:00.000                Canvass    Fail   41.89843137207629   \n",
       "4  2010-01-04T00:00:00.000  License Re-Inspection    Pass   41.93844282365204   \n",
       "\n",
       "            longitude                                           location  \\\n",
       "0  -87.67596676683779  {'latitude': '41.961605669949854', 'longitude'...   \n",
       "1                 NaN                                                NaN   \n",
       "2  -87.67596676683779  {'latitude': '41.961605669949854', 'longitude'...   \n",
       "3   -87.6280091630558  {'latitude': '41.89843137207629', 'longitude':...   \n",
       "4  -87.76831838068422  {'latitude': '41.93844282365204', 'longitude':...   \n",
       "\n",
       "                                          violations  \n",
       "0                                                NaN  \n",
       "1  18. NO EVIDENCE OF RODENT OR INSECT OUTER OPEN...  \n",
       "2                                                NaN  \n",
       "3  18. NO EVIDENCE OF RODENT OR INSECT OUTER OPEN...  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transformar_ingesta(dataset):\n",
    "    return pd.DataFrame.from_dict(dataset)\n",
    "\n",
    "df = transformar_ingesta(dataset)\n",
    "print(f\"DataFrame creado con {df.shape[0]} filas y {df.shape[1]} columnas.\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Identificar faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores faltantes por columna:\n",
      "inspection_id          0\n",
      "dba_name               0\n",
      "aka_name            2432\n",
      "license_              18\n",
      "facility_type       5166\n",
      "risk                  83\n",
      "address                0\n",
      "city                 155\n",
      "state                 63\n",
      "zip                   41\n",
      "inspection_date        0\n",
      "inspection_type        1\n",
      "results                0\n",
      "latitude             975\n",
      "longitude            975\n",
      "location             975\n",
      "violations         77339\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def faltantes(df):\n",
    "    return df.isna().sum()\n",
    "\n",
    "print(\"Valores faltantes por columna:\")\n",
    "print(faltantes(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Eliminar faltantes de latitud y longitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas restantes después de eliminar faltantes: 279463\n"
     ]
    }
   ],
   "source": [
    "def elimina_faltantes_latitud_longitud(cols, df):\n",
    "    return df.dropna(subset=cols)\n",
    "\n",
    "df_limpio = elimina_faltantes_latitud_longitud(['latitude', 'longitude'], df)\n",
    "print(f\"Filas restantes después de eliminar faltantes: {df_limpio.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Imputar faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6m/hyg2m8l17n90b43ntnk_cybh0000gn/T/ipykernel_15840/4241019940.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(value, inplace=True)\n",
      "/var/folders/6m/hyg2m8l17n90b43ntnk_cybh0000gn/T/ipykernel_15840/4241019940.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col].fillna(value, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores faltantes después de la imputación:\n",
      "inspection_id          0\n",
      "dba_name               0\n",
      "aka_name            2419\n",
      "license_               0\n",
      "facility_type          0\n",
      "risk                   0\n",
      "address                0\n",
      "city                 152\n",
      "state                  0\n",
      "zip                    0\n",
      "inspection_date        0\n",
      "inspection_type        1\n",
      "results                0\n",
      "latitude               0\n",
      "longitude              0\n",
      "location               0\n",
      "violations         77064\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def imputar_faltantes(col, value, df):\n",
    "    df[col].fillna(value, inplace=True)\n",
    "\n",
    "columns_to_impute = ['license_', 'zip', 'state', 'facility_type', 'risk']\n",
    "for col in columns_to_impute:\n",
    "    mode_value = df_limpio[col].mode()[0]\n",
    "    imputar_faltantes(col, mode_value, df_limpio)\n",
    "\n",
    "print(\"Valores faltantes después de la imputación:\")\n",
    "print(faltantes(df_limpio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Transformación de enteros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipos de datos después de transformar enteros:\n",
      "inspection_id       int64\n",
      "dba_name           object\n",
      "aka_name           object\n",
      "license_           object\n",
      "facility_type      object\n",
      "risk               object\n",
      "address            object\n",
      "city               object\n",
      "state              object\n",
      "zip                object\n",
      "inspection_date    object\n",
      "inspection_type    object\n",
      "results            object\n",
      "latitude           object\n",
      "longitude          object\n",
      "location           object\n",
      "violations         object\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6m/hyg2m8l17n90b43ntnk_cybh0000gn/T/ipykernel_15840/3307818616.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df[col].astype(int)\n"
     ]
    }
   ],
   "source": [
    "def transformar_enteros(cols, df):\n",
    "    for col in cols:\n",
    "        df[col] = df[col].astype(int)\n",
    "    return df\n",
    "\n",
    "df_limpio = transformar_enteros(['inspection_id'], df_limpio)\n",
    "print(\"Tipos de datos después de transformar enteros:\")\n",
    "print(df_limpio.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Transformación de flotantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipos de datos después de transformar flotantes:\n",
      "inspection_id        int64\n",
      "dba_name            object\n",
      "aka_name            object\n",
      "license_            object\n",
      "facility_type       object\n",
      "risk                object\n",
      "address             object\n",
      "city                object\n",
      "state               object\n",
      "zip                 object\n",
      "inspection_date     object\n",
      "inspection_type     object\n",
      "results             object\n",
      "latitude           float64\n",
      "longitude          float64\n",
      "location            object\n",
      "violations          object\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6m/hyg2m8l17n90b43ntnk_cybh0000gn/T/ipykernel_15840/4228924389.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df[col].astype(float)\n",
      "/var/folders/6m/hyg2m8l17n90b43ntnk_cybh0000gn/T/ipykernel_15840/4228924389.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df[col].astype(float)\n"
     ]
    }
   ],
   "source": [
    "def transformar_flotantes(cols, df):\n",
    "    for col in cols:\n",
    "        df[col] = df[col].astype(float)\n",
    "    return df\n",
    "\n",
    "df_limpio = transformar_flotantes(['latitude', 'longitude'], df_limpio)\n",
    "print(\"Tipos de datos después de transformar flotantes:\")\n",
    "print(df_limpio.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Transformación de fechas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipos de datos después de transformar fechas:\n",
      "inspection_id               int64\n",
      "dba_name                   object\n",
      "aka_name                   object\n",
      "license_                   object\n",
      "facility_type              object\n",
      "risk                       object\n",
      "address                    object\n",
      "city                       object\n",
      "state                      object\n",
      "zip                        object\n",
      "inspection_date    datetime64[ns]\n",
      "inspection_type            object\n",
      "results                    object\n",
      "latitude                  float64\n",
      "longitude                 float64\n",
      "location                   object\n",
      "violations                 object\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6m/hyg2m8l17n90b43ntnk_cybh0000gn/T/ipykernel_15840/2736244655.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = pd.to_datetime(df[col])\n"
     ]
    }
   ],
   "source": [
    "def transformar_fechas(cols, df):\n",
    "    for col in cols:\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "    return df\n",
    "\n",
    "df_limpio = transformar_fechas(['inspection_date'], df_limpio)\n",
    "print(\"Tipos de datos después de transformar fechas:\")\n",
    "print(df_limpio.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Data profiling de variables categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data profiling de variables categóricas:\n",
      "             dba_name  aka_name  license_ facility_type           risk  \\\n",
      "uniques         32809     31210     45534           513              4   \n",
      "prop_uniques   0.1174  0.111678  0.162934      0.001836       0.000014   \n",
      "mode           SUBWAY    SUBWAY         0    Restaurant  Risk 1 (High)   \n",
      "\n",
      "                  city     state  \n",
      "uniques             23         1  \n",
      "prop_uniques  0.000082  0.000004  \n",
      "mode           CHICAGO        IL  \n"
     ]
    }
   ],
   "source": [
    "def data_profiling_string(cols, df):\n",
    "    profiling = {}\n",
    "    for col in cols:\n",
    "        profiling[col] = {\n",
    "            'uniques': df[col].nunique(),\n",
    "            'prop_uniques': df[col].nunique() / len(df),\n",
    "            'mode': df[col].mode()[0]\n",
    "        }\n",
    "    return pd.DataFrame(profiling)\n",
    "\n",
    "cat_cols = ['dba_name', 'aka_name', 'license_', 'facility_type', 'risk', 'city', 'state']\n",
    "print(\"Data profiling de variables categóricas:\")\n",
    "print(data_profiling_string(cat_cols, df_limpio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Data profiling de fechas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data profiling de fechas:\n",
      "                  inspection_date\n",
      "uniques                      3730\n",
      "prop_uniques             0.013347\n",
      "mode          2013-11-14 00:00:00\n",
      "min_date      2010-01-04 00:00:00\n",
      "max_date      2024-10-17 00:00:00\n",
      "unique_years                   15\n",
      "total_days                   5400\n"
     ]
    }
   ],
   "source": [
    "def data_profiling_fechas(cols, df):\n",
    "    profiling = {}\n",
    "    for col in cols:\n",
    "        profiling[col] = {\n",
    "            'uniques': df[col].nunique(),\n",
    "            'prop_uniques': df[col].nunique() / len(df),\n",
    "            'mode': df[col].mode()[0],\n",
    "            'min_date': df[col].min(),\n",
    "            'max_date': df[col].max(),\n",
    "            'unique_years': df[col].dt.year.nunique(),\n",
    "            'total_days': (df[col].max() - df[col].min()).days\n",
    "        }\n",
    "    return pd.DataFrame(profiling)\n",
    "\n",
    "print(\"Data profiling de fechas:\")\n",
    "print(data_profiling_fechas(['inspection_date'], df_limpio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Guardar datos limpios en S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos guardados exitosamente en aplicaciones-cd-1-mcda24a004/limpieza/datos-limpios-2024-10-20.pkl\n",
      "Proceso de guardado completado con éxito.\n",
      "Región AWS utilizada: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pickle\n",
    "from datetime import date\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Se agregan las credenciales ya que al no ser un .py y ser cuenta de estudiante constantemente se deben refrescar\n",
    "\n",
    "# Configurar el cliente de boto3 con las credenciales\n",
    "boto3.setup_default_session(\n",
    "    aws_access_key_id=config['s3']['aws_access_key_id'],\n",
    "    aws_secret_access_key=config['s3']['aws_secret_access_key'],\n",
    "    aws_session_token=config['s3']['aws_session_token']\n",
    ")\n",
    "\n",
    "# Definir la región\n",
    "AWS_REGION = \"us-east-1\"\n",
    "\n",
    "def guardar_datos_s3(df):\n",
    "    bucket = f\"aplicaciones-cd-1-{config['iexe']['matricula']}\"\n",
    "    today = date.today().strftime(\"%Y-%m-%d\")\n",
    "    key = f\"limpieza/datos-limpios-{today}.pkl\"\n",
    "    \n",
    "    s3 = boto3.resource('s3', region_name=AWS_REGION)\n",
    "    \n",
    "    try:\n",
    "        # Serializar el DataFrame\n",
    "        pickle_data = pickle.dumps(df)\n",
    "        \n",
    "        # Subir los datos a S3\n",
    "        s3.Object(bucket, key).put(Body=pickle_data)\n",
    "        print(f\"Datos guardados exitosamente en {bucket}/{key}\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        print(f\"Error al guardar los datos: {e}\")\n",
    "        return False\n",
    "\n",
    "# ejecucion principal\n",
    "if 'df_limpio' in globals():\n",
    "    if guardar_datos_s3(df_limpio):\n",
    "        print(\"Proceso de guardado completado con éxito.\")\n",
    "else:\n",
    "    print(\"Error: 'df_limpio' no está definido. Asegúrate de tener datos limpios para guardar.\")\n",
    "\n",
    "print(f\"Región AWS utilizada: {AWS_REGION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12Escenario de limpieza EXTRA o PLUS que no se agrega a la carga para no modificar el resultado esperado y se incluye para demostrar conocimeinto y entendimeinto del probelma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se detectaron 0 valores atípicos en la columna latitude\n",
      "Se detectaron 3542 valores atípicos en la columna longitude\n",
      "Resumen estadístico después de manejar valores atípicos:\n",
      "            latitude      longitude\n",
      "count  279463.000000  279463.000000\n",
      "mean       41.880622     -87.673153\n",
      "std         0.081011       0.052173\n",
      "min        41.644670     -87.846320\n",
      "25%        41.831542     -87.704734\n",
      "50%        41.891733     -87.666126\n",
      "75%        41.939713     -87.634827\n",
      "max        42.021064     -87.525094\n",
      "\n",
      "Comparación de latitude:\n",
      "Antes:\n",
      "count    279463.000000\n",
      "mean         41.880622\n",
      "std           0.081011\n",
      "min          41.644670\n",
      "25%          41.831542\n",
      "50%          41.891733\n",
      "75%          41.939713\n",
      "max          42.021064\n",
      "Name: latitude, dtype: float64\n",
      "Después:\n",
      "count    279463.000000\n",
      "mean         41.880622\n",
      "std           0.081011\n",
      "min          41.644670\n",
      "25%          41.831542\n",
      "50%          41.891733\n",
      "75%          41.939713\n",
      "max          42.021064\n",
      "Name: latitude, dtype: float64\n",
      "\n",
      "Comparación de longitude:\n",
      "Antes:\n",
      "count    279463.000000\n",
      "mean        -87.676198\n",
      "std           0.058327\n",
      "min         -87.906874\n",
      "25%         -87.707321\n",
      "50%         -87.666126\n",
      "75%         -87.634827\n",
      "max         -87.525094\n",
      "Name: longitude, dtype: float64\n",
      "Después:\n",
      "count    279463.000000\n",
      "mean        -87.673153\n",
      "std           0.052173\n",
      "min         -87.846320\n",
      "25%         -87.704734\n",
      "50%         -87.666126\n",
      "75%         -87.634827\n",
      "max         -87.525094\n",
      "Name: longitude, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nJustificación:\\n1. Detección de valores atípicos: Es crucial identificar y manejar valores atípicos \\n   ya que pueden distorsionar significativamente los análisis estadísticos y los modelos predictivos.\\n\\n2. Flexibilidad en el método: Ofrecemos dos métodos comunes (Z-score e IQR) para \\n   detectar atípicos, permitiendo adaptarse a diferentes distribuciones de datos.\\n\\n3. Transparencia: Informamos sobre la cantidad de valores atípicos detectados, \\n   lo que ayuda a entender mejor la calidad de los datos.\\n\\n4. Manejo conservador: Reemplazamos los valores atípicos por la mediana de la columna, \\n   lo cual es una estrategia conservadora que preserva la distribución general de los datos.\\n\\n5. Comparación antes/después: Mostramos estadísticas descriptivas antes y después del \\n   tratamiento, lo que permite evaluar el impacto de la limpieza.\\n\\n6. Mejora en la calidad de datos: Al manejar los valores atípicos, mejoramos la \\n   calidad general de los datos para análisis posteriores y modelado.\\n\\nEsta adición complementa el trabajo de limpieza ya realizado, abordando un aspecto \\nimportante que no se había cubierto previamente y que es una práctica común en \\npreparación de datos para análisis y modelado en ciencia de datos.\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 12. Detección y manejo de valores atípicos\n",
    "\n",
    "def detectar_y_manejar_atipicos(df, columnas_numericas, metodo='zscore', umbral=3):\n",
    "    \"\"\"\n",
    "    Detecta y maneja valores atípicos en las columnas numéricas especificadas.\n",
    "    \n",
    "    Args:\n",
    "    df (pandas.DataFrame): El DataFrame a procesar.\n",
    "    columnas_numericas (list): Lista de columnas numéricas para analizar.\n",
    "    metodo (str): Método para detectar atípicos ('zscore' o 'iqr').\n",
    "    umbral (float): Umbral para considerar un valor como atípico.\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame con los valores atípicos manejados.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from scipy import stats\n",
    "    \n",
    "    df_limpio = df.copy()\n",
    "    \n",
    "    for columna in columnas_numericas:\n",
    "        if metodo == 'zscore':\n",
    "            # Método Z-score\n",
    "            z_scores = np.abs(stats.zscore(df_limpio[columna]))\n",
    "            mask = z_scores > umbral\n",
    "        elif metodo == 'iqr':\n",
    "            # Método IQR (Rango Intercuartil)\n",
    "            Q1 = df_limpio[columna].quantile(0.25)\n",
    "            Q3 = df_limpio[columna].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - umbral * IQR\n",
    "            upper_bound = Q3 + umbral * IQR\n",
    "            mask = (df_limpio[columna] < lower_bound) | (df_limpio[columna] > upper_bound)\n",
    "        else:\n",
    "            raise ValueError(\"Método no reconocido. Use 'zscore' o 'iqr'.\")\n",
    "        \n",
    "        # Conteo de valores atípicos\n",
    "        n_outliers = mask.sum()\n",
    "        print(f\"Se detectaron {n_outliers} valores atípicos en la columna {columna}\")\n",
    "        \n",
    "        # Manejar valores atípicos (en este caso, los reemplazamos por la mediana)\n",
    "        df_limpio.loc[mask, columna] = df_limpio[columna].median()\n",
    "    \n",
    "    return df_limpio\n",
    "\n",
    "# Aplicar la función a nuestro DataFrame\n",
    "columnas_numericas = ['latitude', 'longitude']  # Añadir otras columnas numéricas según sea necesario\n",
    "df_sin_atipicos = detectar_y_manejar_atipicos(df_limpio, columnas_numericas)\n",
    "\n",
    "print(\"Resumen estadístico después de manejar valores atípicos:\")\n",
    "print(df_sin_atipicos[columnas_numericas].describe())\n",
    "\n",
    "# Comparación antes y después\n",
    "for col in columnas_numericas:\n",
    "    print(f\"\\nComparación de {col}:\")\n",
    "    print(\"Antes:\")\n",
    "    print(df_limpio[col].describe())\n",
    "    print(\"Después:\")\n",
    "    print(df_sin_atipicos[col].describe())\n",
    "\n",
    "\"\"\"\n",
    "Justificación:\n",
    "1. Detección de valores atípicos: Es crucial identificar y manejar valores atípicos \n",
    "   ya que pueden distorsionar significativamente los análisis estadísticos y los modelos predictivos.\n",
    "\n",
    "2. Flexibilidad en el método: Ofrecemos dos métodos comunes (Z-score e IQR) para \n",
    "   detectar atípicos, permitiendo adaptarse a diferentes distribuciones de datos.\n",
    "\n",
    "3. Transparencia: Informamos sobre la cantidad de valores atípicos detectados, \n",
    "   lo que ayuda a entender mejor la calidad de los datos.\n",
    "\n",
    "4. Manejo conservador: Reemplazamos los valores atípicos por la mediana de la columna, \n",
    "   lo cual es una estrategia conservadora que preserva la distribución general de los datos.\n",
    "\n",
    "5. Comparación antes/después: Mostramos estadísticas descriptivas antes y después del \n",
    "   tratamiento, lo que permite evaluar el impacto de la limpieza.\n",
    "\n",
    "6. Mejora en la calidad de datos: Al manejar los valores atípicos, mejoramos la \n",
    "   calidad general de los datos para análisis posteriores y modelado.\n",
    "\n",
    "Esta adición complementa el trabajo de limpieza ya realizado, abordando un aspecto \n",
    "importante que no se había cubierto previamente y que es una práctica común en \n",
    "preparación de datos para análisis y modelado en ciencia de datos.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
